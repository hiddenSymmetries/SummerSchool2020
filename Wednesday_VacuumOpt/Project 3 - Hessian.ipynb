{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Ill-posedness and Tikhonov regularisation\n",
    "\n",
    "You may have noticed, that the optimisation routine needs many thousand steps to converge. In this project we will investigate why this is the case.\n",
    "\n",
    "Let's start by setting up our optimisation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyplasmaopt import *\n",
    "nfp = 2\n",
    "(coils, currents, expansion_axis, eta_bar) = get_24_coil_data(nfp=nfp, ppp=10, at_optimum=False)\n",
    "currents = [1e5 * x for x in   [-2.271314992875459, -2.223774477156286, -2.091959078815509, -1.917569373937265, -2.115225147955706, -2.025410501731495]]\n",
    "stellarator = CoilCollection(coils, currents, nfp, True)\n",
    "iota_target = 0.103\n",
    "coil_length_target = 4.398229715025710\n",
    "magnetic_axis_length_target = 6.356206812106860\n",
    "eta_bar = -2.25\n",
    "obj = SimpleNearAxisQuasiSymmetryObjective(\n",
    "        stellarator, expansion_axis, iota_target, eta_bar=eta_bar,\n",
    "        coil_length_target=coil_length_target, magnetic_axis_length_target=magnetic_axis_length_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code can only compute the objective value and its first derivative, we will have to use finite differences to approximate the Hessian matrix.\n",
    "The $i$-th column of the Hessian $H$ can be approximated via\n",
    "$$\\tilde H(:, i) = \\frac{\\nabla f(x+\\epsilon e_i) - \\nabla f(x-\\epsilon e_i)}{2\\epsilon}$$\n",
    "where $e_i$ is the $i$-th unit vector and for some small $\\epsilon$ (e.g. $\\epsilon=10^{-5}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1) Write a function that approximates the Hessian at a given point $x$. By Schwarz's theorem, we know that the Hessian should be a symmetric matrix. Is this the case for the approximation that you computed? If not, how can we 'symmetrize' the Hessian approximation?\n",
    "\n",
    "2) Run the optimisation for 2000 iterations and then compute the eigenvalues of the Hessian approximation at the minimum. Plot the eigenvalues. What does it mean for the objective when some eigenvalues are very small, and some are very large? What does this mean for the performance of gradient descent vs (quasi-)Newton methods?\n",
    "\n",
    "3) A popular method of regularising ill-posed objectives, is the so called Tikhonov regularisation. Instead of solving \n",
    "$$ \\min_x f(x) $$\n",
    "one solves \n",
    "$$ \\min_x f(x) + \\frac\\alpha2 \\| x-x_0\\|^2 $$\n",
    "where $\\alpha>0$ and $x_0$ is the initial guess. Implement a Tikhonov regularisation and rerun the optimisation for varying values of $\\alpha$. Look at the obtained objective value, coils, and eigenvalues of the Hessian.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_fun(x):\n",
    "    obj.update(x)\n",
    "    res = obj.res\n",
    "    dres = obj.dres\n",
    "    return res, dres\n",
    "\n",
    "def compute_hessian(fun, x, eps=1e-5):\n",
    "    n = x.size\n",
    "    H = np.zeros((n, n))\n",
    "    x0 = x\n",
    "    for i in range(n):\n",
    "        xp = x0.copy()\n",
    "        xp[i] = x0[i] + eps\n",
    "        d1 = fun(xp)[1]\n",
    "        xp[i] = x0[i] - eps\n",
    "        d0 = fun(xp)[1]\n",
    "        H[i, :] = (d1-d0)/(2*eps)\n",
    "    H = 0.5 * (H+H.T) # To obtain a symmetric matrix.\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "obj.clear_history()\n",
    "res = minimize(scipy_fun, obj.x0, jac=True, method='bfgs', tol=1e-20,\n",
    "               options={\"maxiter\": 2000},\n",
    "               callback=obj.callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = compute_hessian(scipy_fun, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradientnorm at minimum =', np.linalg.norm(res.jac))\n",
    "from scipy.linalg import eigh\n",
    "evals = eigh(H, eigvals_only=True)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(np.abs(evals))\n",
    "plt.title('Eigenvalues')\n",
    "plt.show()\n",
    "plt.semilogy(obj.Jvals)\n",
    "plt.title('Convergence')\n",
    "plt.show()\n",
    "plot_stellarator(stellarator, axis=expansion_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue differ by several orders of magnitude. This means, that the objective varies strongly in some directions, but barely in others. Standard gradient descent methods struggle for these problems. Quasi-Newton methods such as BFGS can learn the local structue of the objective and will chose steps that are appropriately scaled in the directions of large and small eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-1\n",
    "x0 = obj.x0.copy()\n",
    "def scipy_fun_tikhonov(x):\n",
    "    obj.update(x)\n",
    "    res = obj.res\n",
    "    dres = obj.dres\n",
    "    res += 0.5 * alpha * np.sum((x-x0)**2)\n",
    "    dres += alpha * (x-x0)\n",
    "    return res, dres\n",
    "\n",
    "obj.clear_history()\n",
    "res_tik = minimize(scipy_fun_tikhonov, obj.x0, jac=True, method='bfgs', tol=1e-20,\n",
    "               options={\"maxiter\": 2000},\n",
    "               callback=obj.callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Htik = compute_hessian(scipy_fun_tikhonov, res_tik.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient at minimum =', np.linalg.norm(res_tik.jac))\n",
    "evals_tik = eigh(Htik, eigvals_only=True)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(np.abs(evals_tik))\n",
    "plt.title('Eigenvalues')\n",
    "plt.show()\n",
    "plt.semilogy(obj.Jvals)\n",
    "plt.title('Objective value')\n",
    "plt.show()\n",
    "plot_stellarator(stellarator, axis=expansion_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: \n",
    "For larger $\\alpha$, we can not drive the objective as close to zero. However, it becomes easier to get a small gradient. In addition, we observe more regular coils for larger $\\alpha$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
