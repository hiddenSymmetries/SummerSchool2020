{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Ill-posedness and Tikhonov regularisation\n",
    "\n",
    "You may have noticed, that the optimisation routine needs many thousand steps to converge. In this project we will investigate why this is the case.\n",
    "\n",
    "Let's start by setting up our optimisation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyplasmaopt import *\n",
    "nfp = 2\n",
    "(coils, currents, expansion_axis, eta_bar) = get_24_coil_data(nfp=nfp, ppp=8, at_optimum=False)\n",
    "currents = [1e5 * x for x in   [-2.271314992875459, -2.223774477156286, -2.091959078815509, -1.917569373937265, -2.115225147955706, -2.025410501731495]]\n",
    "stellarator = CoilCollection(coils, currents, nfp, True)\n",
    "iota_target = 0.103\n",
    "coil_length_target = 4.398229715025710\n",
    "magnetic_axis_length_target = 6.356206812106860\n",
    "eta_bar = -2.25\n",
    "obj = SimpleNearAxisQuasiSymmetryObjective(\n",
    "        stellarator, expansion_axis, iota_target, eta_bar=eta_bar,\n",
    "        coil_length_target=coil_length_target, magnetic_axis_length_target=magnetic_axis_length_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code can only compute the objective value and its first derivative, we will have to use finite differences to approximate the Hessian matrix.\n",
    "The $i$-th column of the Hessian $H$ can be approximated via\n",
    "$$ H(:, i) \\approx \\frac{\\nabla f(x+\\epsilon e_i) - \\nabla f(x-\\epsilon e_i)}{2\\epsilon}$$\n",
    "where $e_i$ is the $i$-th unit vector and for some small $\\epsilon$ (e.g. $\\epsilon=10^{-5}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1) Write a function that approximates the Hessian at a given point $x$. By Schwarz's theorem, we know that the Hessian should be a symmetric matrix. Is this the case for the approximation that you computed? If not, how can we 'symmetrize' the Hessian approximation?\n",
    "\n",
    "2) Run the optimisation for 2000 iterations and then compute the eigenvalues of the Hessian approximation at the minimum. Plot the eigenvalues. What does it mean for the objective when some eigenvalues are very small, and some are very large? What does this mean for the performance of gradient descent vs (quasi-)Newton methods?\n",
    "\n",
    "3) A popular method of regularising ill-posed objectives, is the so called Tikhonov regularisation. Instead of solving \n",
    "$$ \\min_x f(x) $$\n",
    "one solves \n",
    "$$ \\min_x f(x) + \\frac\\alpha2 \\| x-x_0\\|^2 $$\n",
    "where $\\alpha>0$ and $x_0$ is the initial guess. Implement a Tikhonov regularisation and rerun the optimisation for varying values of $\\alpha$. Look at the obtained objective value, coils, and eigenvalues of the Hessian.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
