{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Finite differences\n",
    "\n",
    "Many physics codes only provide objective values and are not able to compute their gradients. In that case, the derivative is often approximated using finite differences. The goal of this project is to implement your own finite difference approximation and investigate the impact of round-off error as well as the increase in runtime compared to analytical gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets begin by setting up our toy problem again.\n",
    "from pyplasmaopt import *\n",
    "nfp = 2\n",
    "(coils, currents, magnetic_axis, eta_bar) = get_24_coil_data(nfp=nfp, ppp=10, at_optimum=False)\n",
    "stellarator = CoilCollection(coils, currents, nfp, True)\n",
    "iota_target = 0.103\n",
    "coil_length_target = 4.398229715025710\n",
    "magnetic_axis_length_target = 6.356206812106860\n",
    "eta_bar = -2.25\n",
    "obj = SimpleNearAxisQuasiSymmetryObjective(\n",
    "        stellarator, magnetic_axis, iota_target, eta_bar=eta_bar,\n",
    "        coil_length_target=coil_length_target, magnetic_axis_length_target=magnetic_axis_length_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first order approximation to the derivative at $x$ in direction of the $i$-th degree of freedom is given by\n",
    "$$ (\\nabla f(x))_i= \\frac{f(x+\\epsilon e_i) - f(x)}{\\epsilon} + O(\\epsilon). $$\n",
    "\n",
    "The following code will compute the gradient in a direction $h$ and compares it to the derivative that the code computes for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_directional_derivative_fd(obj, x, h, eps):\n",
    "    obj.update(x, compute_derivative=False)\n",
    "    fx = obj.res\n",
    "    obj.update(x+eps*h, compute_derivative=False)\n",
    "    fxh = obj.res\n",
    "    return (fxh-fx)/eps\n",
    "\n",
    "x = obj.x0\n",
    "h = np.random.rand(*(x.shape))\n",
    "obj.update(x)\n",
    "derivative = np.sum(obj.dres*h)\n",
    "eps = 1e-6\n",
    "fd_derivative = compute_directional_derivative_fd(obj, x, h, eps)\n",
    "print('Exact derivative =', derivative)\n",
    "print('Finite differences derivative =', fd_derivative)\n",
    "print('Error =', abs(derivative-fd_derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Vary the stepsize $\\epsilon$. As $\\epsilon\\to0$, how does the error behave?\n",
    "2. Look up the 2nd, 4th, and 6th order finite difference stencils on wikipedia and implement them. Show a plot comparing the step size $\\epsilon$ with the approximation error. What do you observe?\n",
    "3. The `minimize` function in scipy supports finite differences to compute the gradient. To use this, set the argument `jac=False` and make sure that the function you pass to minimise only returns the objective value, and not both objective and gradient. Now run the optimisation once with exact gradients and once using the scipy finite difference approximation for 500 iterations. Compare the runtime of the two and plot the objective values. What to do you observe? Call `print` on the object that `minimize` returns to find out why each algorithm terminated.\n",
    "\n",
    "*Note*: If you pass `callback=obj.callback` to `minimize`, then the objective history will be stored in `obj.Jvals`. Beore running a second optimisation, you can reset this history by calling `obj.clear_history()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [10**(-i) for i in range(5, 14)]:\n",
    "    fd_derivative = compute_directional_derivative_fd(obj, x, h, eps)\n",
    "    err = abs(derivative-fd_derivative)\n",
    "    print(eps, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_directional_derivative_fd_ho(obj, x, h, eps, order):\n",
    "    if order == 1:\n",
    "        shifts = [0, 1]\n",
    "        weights = [-1, 1]\n",
    "    elif order == 2:\n",
    "        shifts = [-1, 1]\n",
    "        weights = [-0.5, 0.5]\n",
    "    elif order == 4:\n",
    "        shifts = [-2, -1, 1, 2]\n",
    "        weights = [1/12, -2/3, 2/3, -1/12]\n",
    "    elif order == 6:\n",
    "        shifts = [-3, -2, -1, 1, 2, 3]\n",
    "        weights = [-1/60, 3/20, -3/4, 3/4, -3/20, 1/60]\n",
    "    obj.update(x + shifts[0]*eps*h, compute_derivative=False)\n",
    "    fd = weights[0] * obj.res\n",
    "    for i in range(1, len(shifts)):\n",
    "        obj.update(x + shifts[i]*eps*h, compute_derivative=False)\n",
    "        fd += weights[i] * obj.res\n",
    "    return fd/eps\n",
    "\n",
    "err1 = []\n",
    "err2 = []\n",
    "err4 = []\n",
    "err6 = []\n",
    "epss = [2**(-i) for i in range(10, 40)]\n",
    "for eps in epss:\n",
    "    fd1 = compute_directional_derivative_fd_ho(obj, x, h, eps, 1)\n",
    "    err1.append(abs(derivative-fd1))\n",
    "    fd2 = compute_directional_derivative_fd_ho(obj, x, h, eps, 2)\n",
    "    err2.append(abs(derivative-fd2))\n",
    "    fd4 = compute_directional_derivative_fd_ho(obj, x, h, eps, 4)\n",
    "    err4.append(abs(derivative-fd4))\n",
    "    fd6 = compute_directional_derivative_fd_ho(obj, x, h, eps, 6)\n",
    "    err6.append(abs(derivative-fd6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(epss, err1, label=\"1st order\")\n",
    "plt.loglog(epss, err2, label=\"2nd order\")\n",
    "plt.loglog(epss, err4, label=\"4th order\")\n",
    "plt.loglog(epss, err6, label=\"6th order\")\n",
    "plt.legend()\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.optimize import minimize\n",
    "obj.clear_history()\n",
    "def scipy_fun_fd(x):\n",
    "    obj.update(x, compute_derivative=False)\n",
    "    res = obj.res\n",
    "    return res\n",
    "\n",
    "res = minimize(scipy_fun_fd, obj.x0, jac=False, method='bfgs', tol=1e-20,\n",
    "               options={\"maxiter\": 500},\n",
    "               callback=obj.callback)\n",
    "print(res)\n",
    "convergence_fd = obj.Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "obj.clear_history()\n",
    "def scipy_fun(x):\n",
    "    obj.update(x)\n",
    "    res = obj.res\n",
    "    dres = obj.dres\n",
    "    return res, dres\n",
    "\n",
    "res = minimize(scipy_fun, obj.x0, jac=True, method='bfgs', tol=1e-20,\n",
    "               options={\"maxiter\": 500},\n",
    "               callback=obj.callback)\n",
    "convergence_exact = obj.Jvals\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(convergence_exact, label='Exact derivative')\n",
    "plt.semilogy(convergence_fd, label='Finite difference approximation')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm using finite differences fails after ~300 iterations due to loss of precision! Using exact derivatives, the algorithm keeps going and is able to reduce the objective further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
