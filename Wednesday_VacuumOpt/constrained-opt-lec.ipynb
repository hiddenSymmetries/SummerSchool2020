{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "As in our lectures on unconstrained optimization, we consider a smooth objective/cost function $\\phi:\\mathbb R^n\\to \\mathbb R$ and the following problem\n",
    "\n",
    "$$\n",
    "  \\min_{x \\in \\Omega} \\phi(x)\n",
    "$$\n",
    "\n",
    "To define $\\Omega$, we consider a smooth function $c:\\mathbb R^n\\to\\mathbb R^m$, $m\\ge 1$, and either the equality constraint set\n",
    "\n",
    "$$\n",
    "    \\Omega:=\\{x\\in \\mathbb R^n: c(x) = 0\\},\n",
    "$$\n",
    "\n",
    "or the inequality constraint set (where the inequality is understood in a coordinate-wise sense):\n",
    "\n",
    "$$\n",
    "    \\Omega:=\\{x\\in \\mathbb R^n: c(x) \\le 0\\}.\n",
    "$$\n",
    "\n",
    "Since in our minimization, we are only allowed points in $\\Omega$, the necessary condition for local optimia from unconstrained optimization, $\\nabla \\phi = 0$ is not valid anymore. We will discuss the following approaches:\n",
    "\n",
    "- Quadratic regularization\n",
    "- Logarithmic barriers\n",
    "- Forward/adjoint sensitivity for (implicit equation) equality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Simple model problem\n",
    "\n",
    "Let's look at a simple two-dimensional model problem\n",
    "\n",
    "$$\n",
    "    \\min_{x=(x_1,x_2)} x_1+x_2\n",
    "$$\n",
    "\n",
    "subject to the equality constraint\n",
    "\n",
    "$$\n",
    "    c(x) = x_1^2 + x_2^2-2 = 0\n",
    "$$\n",
    "\n",
    "or subject to the inequality constraint\n",
    "\n",
    "$$\n",
    "  c(x) = x_1^2 + x_2^2-2 \\le 0\n",
    "$$\n",
    "\n",
    "The constraint describes a circle centered at the origin. We will see that the minimum of both, the equality as well as the inequality constrained optimization problem is attained at $x^*=(-\\sqrt{2},-\\sqrt{2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,y):\n",
    "    \"Simple linear objective\"\n",
    "    return x + y\n",
    "\n",
    "def constraint(x,y):\n",
    "    \"Constraint describing a circle\"\n",
    "    return x**2 + y**2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phi_constraint():\n",
    "    \"Draw a filled contour plot\"\n",
    "    xx = np.linspace(-1.8, 1.8)\n",
    "    X, Y = np.meshgrid(xx, xx)\n",
    "    Z = phi(X,Y)\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    cp = ax.contourf(X, Y, Z, 20)\n",
    "    circle = plt.Circle((0, 0), np.sqrt(2), color='r', fill=False)\n",
    "    fig.colorbar(cp)\n",
    "    ax.add_artist(circle)\n",
    "    ax.set_aspect('equal')\n",
    "    return fig, ax\n",
    "\n",
    "plot_phi_constraint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic penalization\n",
    "\n",
    "Let us now attempts to solve this constrained optimization problem numerically.\n",
    "\n",
    "### Equality constraints\n",
    "First, we consider the equality-constrained problem. To remove the constraint, we use a quadratic penalization with a parameter $\\mu>0$ and consider the (unconstrained!) problem\n",
    "\n",
    "$$\n",
    "\\min_{x\\in \\mathbb R^n} Q(x,\\mu), \\quad \\text{where}\\quad Q(x,\\mu) := \\phi(x) + \\frac{1}{2\\mu} \\|c(x)\\|^2 \n",
    "$$\n",
    "\n",
    "By choosing small $\\mu$, we penalize the constraint violation---the smaller $\\mu$, the more emphasis we put on satisfying the constraint. However, for most problems, the constraint is only exactly satisfied if the penalization paramter $\\mu\\to 0$. Let us look at the contour lines of the regularized objective for different values of $\\mu$. It can be seen that for smaller $\\mu$, the minimum of $Q(\\cdot,\\mu)$ becomes a better approximation of the true minimum, i.e., the minimum of the constrained problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_quadratic(x,y,mu):\n",
    "    \"Quadratic penalization of objective\"\n",
    "    pen = constraint(x,y)**2\n",
    "    return phi(x,y) + 1/(2*mu)*pen\n",
    "\n",
    "def plot_phi_quadratic(mu):\n",
    "    xx = np.linspace(-1.75, 1.75)\n",
    "    X, Y = np.meshgrid(xx, xx)\n",
    "    Z = phi_quadratic(X,Y,mu)\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    cp = ax.contourf(X, Y, Z, 20)\n",
    "    circle = plt.Circle((0, 0), np.sqrt(2), color='r', fill=False)\n",
    "    fig.colorbar(cp)\n",
    "    ax.add_artist(circle)\n",
    "    ax.set_aspect('equal')\n",
    "    return fig, ax\n",
    "\n",
    "plot_phi_quadratic(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inequality constraint\n",
    "\n",
    "A similar trick also works for inequality constraints $c(x)\\le 0$. To remove this inequality constraint, we again use a quadratic penalization with a parameter $\\mu>0$ and consider\n",
    "\n",
    "$$\n",
    "\\min_{x\\in \\mathbb R^n} \\bar Q(x,\\mu), \\quad \\text{where}\\quad \\bar Q(x,\\mu) := \\phi(x) + \\frac{1}{2\\mu} \\|\\max(0,c(x))\\|^2, \n",
    "$$\n",
    "\n",
    "where the max-function acts on each component (there's only one component in our simple test function). Thus, if the inequality constraint is satisfied, $\\phi$ and $\\bar Q$ coincide. If not, the violation of the constraint is penalized. Note that for fixed $\\mu$, the objective $\\bar Q(\\cdot )$ is only once continuously differentiable due to the max-function. Let's look at the contours of the penalized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barphi_quadratic(x,y,mu):\n",
    "    \"Quadratic penalization of objective\"\n",
    "    pen = np.maximum(0,constraint(x,y))**2\n",
    "    return phi(x,y) + 1/(2*mu)*pen\n",
    "\n",
    "def plot_barphi_quadratic(mu):\n",
    "    xx = np.linspace(-1.75, 1.75)\n",
    "    X, Y = np.meshgrid(xx, xx)\n",
    "    Z = barphi_quadratic(X,Y,mu)\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    cp = ax.contourf(X, Y, Z, 20)\n",
    "    circle = plt.Circle((0, 0), np.sqrt(2), color='r', fill=False)\n",
    "    fig.colorbar(cp)\n",
    "    ax.add_artist(circle)\n",
    "    ax.set_aspect('equal')\n",
    "    return fig, ax\n",
    "\n",
    "plot_barphi_quadratic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the quadratically regularized objective becomes more difficult to minimize due to narrow, steep valleys in the presence of equality constraints, or simply steep slopes close to the minimizer. As we've seen, this is challenging for unconstrained optimization. Hence, one often performs a continuation procedure as follows:\n",
    "\n",
    "1. Choose $\\mu$ relatively large and an initialization $x^0$\n",
    "2. Solve the quadratically regularized optimization problem starting from $x^0$ to find solution $x^*_\\mu$\n",
    "3. If the constrait violation is small, STOP. Otherwise, reduce $\\mu$ (e.g., half it), set initialization $x^0:=x^*_\\mu$ and go to 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log barriers for inequality constraints\n",
    "\n",
    "The solutions of quadratically regularized problems with inequality constraints are typically *infeasible*, i.e., they are outside the allowed region and only satisfy the inequality constraint in the limit $\\mu\\to 0$. Log barriers are an alternative that ensures that solutions (and also iterates) are always *feasible*, i.e., they satisfy the inequality constraint. The idea is to replace the inequality constraint $c(x)\\le 0$ with a barrier term that is added to the objective as follows (components of $c(x)$ are denoted with index $i$):\n",
    "\n",
    "$$\n",
    "\\min_{x\\in \\mathbb R^n} \\bar P(x,\\mu), \\quad \\text{where}\\quad \\bar P(x,\\mu) := \\phi(x) - \\mu \\sum_i \\log(-c(x)_i).\n",
    "$$\n",
    "\n",
    "Note that $-\\log(-c(x)_i)$ goes to $+\\infty$ as $c(x)_i\\to 0-$. Thus, the barrier term blows up as we get close to the boundary of the feasible region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barphi_log(x,y,mu):\n",
    "    \"Log penalization of objective\"\n",
    "    pen = - (np.log(-constraint(x,y)))\n",
    "    return phi(x,y) + mu*pen\n",
    "\n",
    "def plot_barphi_log(mu):\n",
    "    xx = np.linspace(-1.75, 1.75)\n",
    "    X, Y = np.meshgrid(xx, xx)\n",
    "    Z = barphi_log(X,Y,mu)\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    cp = ax.contourf(X, Y, Z, 20)\n",
    "    circle = plt.Circle((0, 0), np.sqrt(2), color='r', fill=False)\n",
    "    fig.colorbar(cp)\n",
    "    ax.add_artist(circle)\n",
    "    ax.set_aspect('equal')\n",
    "    return fig, ax\n",
    "\n",
    "plot_barphi_log(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, for small $\\mu$ we obtain very large gradients close to the minimizer, which makes this challenging for optimization algorithms (e.g., steepest descent, Newton or quasi Newton method). Thus, one usually uses a similar procedure to carefully decrease $\\mu$ as sketched above. More sophisticated variants of the barrier method exist and are known as interior point methods. Their name indicates that iterates are always in the interior of the  feasible set, and these algorithms provide a systematic way to decrease $\\mu$ and to also solve the optimization problems for each $\\mu$ in a stable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward/adjoint sensitivity analysis\n",
    "\n",
    "See slides and Andrew's lab examples.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
