{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "We consider a smooth objective/cost function $\\phi:\\mathbb R^n\\times \\mathbb R^r\\to \\mathbb R$ that depends on design/control parameters $x$ and random parameters $\\xi$:\n",
    "\n",
    "$$\n",
    "  \\phi(x,\\xi)\n",
    "$$\n",
    "\n",
    "We cannot directly optimize over $x$ as that function still depends on the random variable $\\xi$ which follows a distribution. \n",
    "Below, we discuss some aspects of stochastic optimization:\n",
    "\n",
    "- Scalarization: risk-neutral, risk-averse and robust\n",
    "\n",
    "For fixed $x$, we can take random draws from the distribution $\\phi(x,\\xi)$ and plot the resulting as a histogram. This is commonly known as the stochastic average approximation (SAA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Simple model problem\n",
    "\n",
    "Let's look at a simple model function where $x=(x_1,x_2)$ and $\\xi=(\\xi_1,\\xi_2)$ is a random variable:\n",
    "\n",
    "$$\n",
    "    \\phi(x,\\xi):= (1+\\xi_1)x_1^2 - x_2 - \\log(-x_2) + \\xi_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x1,x2,xi1,xi2):\n",
    "    \"Objective involving uncertainties xi1, xi2\"\n",
    "    return (1+xi1)*x1**2 - x2 - np.log(-x2) + xi2**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phi(x1,x2):\n",
    "    \"Draw histogram for fixed x1,x2\"\n",
    "    no_samp = 100\n",
    "    XI1 = np.random.uniform(-0.1,0.1,no_samp)\n",
    "    XI2 = np.random.uniform(-0.5,0.5,no_samp)\n",
    "    #XI2 = np.random.normal(0,0.3,no_samp)\n",
    "    P = phi(x1,x2,XI1,XI2)\n",
    "    P_mean = np.mean(P)\n",
    "    P_var = np.var(P)\n",
    "    P_max = np.amax(P)\n",
    "    print('Pmean:',P_mean)\n",
    "    print('Pvar:',P_var)\n",
    "    print('Pmax:',P_max)\n",
    "    count, bins, ignored = plt.hist(P, 15, density=True)\n",
    "    plt.xlabel('objective value')\n",
    "    plt.show()\n",
    "    \n",
    "plot_phi(1,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives for optimization\n",
    "\n",
    "From this sampled distribution, we can compute various scalar quantities to be used in a minimization over $x$, which we kept fixed above:\n",
    "\n",
    "For fixed $x$, we can consider:\n",
    "\n",
    "1. **Risk-neutral** formulation: Take expectation over $\\xi$:\n",
    "\n",
    "    $$\n",
    "    \\min_{x} \\mathbb E_{\\xi} \\phi(x,\\xi)\n",
    "    $$\n",
    "\n",
    "2. **Risk-averse** formulations: Avoid poor outcomes, e.g.,\n",
    "    keep variance small:\n",
    "    \n",
    "    $$\n",
    "    \\min_{x} \\mathbb E_{\\xi} \\phi(x,\\xi) + \\mathbb\n",
    "    V_{\\xi}\\phi(x,\\xi)\n",
    "    $$\n",
    "    \n",
    "3. **Robust** formulations: Minimize worst outcome (no risk!)\n",
    "    \n",
    "    $$\n",
    "    \\min_{x} \\max_{\\xi} \\phi(x,\\xi)\n",
    "    $$\n",
    "\n",
    "Let us implement the risk-neutral objective for our test problem defined above. Since the objective simply amounts to a sum of terms, the gradient can easily be computed by hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def phi_risk_neutral(x1,x2):\n",
    "    \"Draw histogram for fixed x1,x2\"\n",
    "    no_samp = 100\n",
    "    np.random.seed(12345)\n",
    "    XI1 = np.random.uniform(-0.1,0.1,no_samp)\n",
    "    XI2 = np.random.uniform(-0.5,0.5,no_samp)\n",
    "    #XI2 = np.random.normal(0,0.3,no_samp)\n",
    "    P = phi(x1,x2,XI1,XI2)\n",
    "    P_mean = np.mean(P)\n",
    "    return P_mean\n",
    "\n",
    "def phi_risk_neutral_grad(x1,x2):\n",
    "    \"Draw histogram for fixed x1,x2\"\n",
    "    no_samp = 100\n",
    "    np.random.seed(12345)\n",
    "    XI1 = np.random.uniform(-0.1,0.1,no_samp)\n",
    "    XI2 = np.random.uniform(-0.5,0.5,no_samp)\n",
    "    #XI2 = np.random.normal(0,0.3,no_samp)\n",
    "    g1 = 2*np.mean(1 + XI1)*x1\n",
    "    g2 = -1 - 1/x2\n",
    "    return  np.array([g1,g2])\n",
    "\n",
    "print(phi_risk_neutral(1,-1))\n",
    "print(phi_risk_neutral_grad(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have all the information you need to run a deterministic optimization method. \n",
    "\n",
    "### Task 1 \n",
    "\n",
    "Verify gradient using finite differences\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Use a steepest descent method (or BFGS) to minimize the risk-neutral function $\\phi_{\\text{RN}}(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
